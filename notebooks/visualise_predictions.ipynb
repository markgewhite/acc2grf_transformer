{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACC → GRF Transformer: Prediction Visualization\n",
    "\n",
    "This notebook provides interactive visualization of model predictions and attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.data_loader import CMJDataLoader\n",
    "from src.transformer import SignalTransformer\n",
    "from src.evaluate import evaluate_model, print_evaluation_summary\n",
    "from src.biomechanics import compute_jump_height, compute_peak_power\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = CMJDataLoader(use_resultant=True)\n",
    "train_ds, val_ds, info = loader.create_datasets(test_size=0.2, batch_size=32)\n",
    "\n",
    "print(f\"Training samples: {info['n_train_samples']}\")\n",
    "print(f\"Validation samples: {info['n_val_samples']}\")\n",
    "print(f\"Input shape: {info['input_shape']}\")\n",
    "print(f\"Output shape: {info['output_shape']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model (update path as needed)\n",
    "MODEL_PATH = '../outputs/checkpoints/best_model.keras'\n",
    "\n",
    "# Or train a quick model for testing\n",
    "TRAIN_NEW_MODEL = True\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "    from src.transformer import build_signal_transformer\n",
    "    model = build_signal_transformer(\n",
    "        seq_len=500,\n",
    "        input_dim=1,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "    )\n",
    "    # Quick training for visualization\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract validation data\n",
    "X_val_list, y_val_list = [], []\n",
    "for X_batch, y_batch in val_ds:\n",
    "    X_val_list.append(X_batch.numpy())\n",
    "    y_val_list.append(y_batch.numpy())\n",
    "X_val = np.concatenate(X_val_list, axis=0)\n",
    "y_val = np.concatenate(y_val_list, axis=0)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_normalized = model.predict(X_val, verbose=0)\n",
    "\n",
    "# Denormalize\n",
    "y_val_bw = loader.denormalize_grf(y_val)\n",
    "y_pred_bw = loader.denormalize_grf(y_pred_normalized)\n",
    "\n",
    "print(f\"Predictions shape: {y_pred_bw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Predicted vs Actual GRF Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grf_comparison(y_true, y_pred, indices=None, n_samples=5):\n",
    "    \"\"\"Plot predicted vs actual GRF curves.\"\"\"\n",
    "    if indices is None:\n",
    "        indices = np.random.choice(len(y_true), min(n_samples, len(y_true)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 1, figsize=(12, 3*len(indices)))\n",
    "    if len(indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = axes[i]\n",
    "        time = np.arange(500)\n",
    "        \n",
    "        actual = y_true[idx].flatten()\n",
    "        predicted = y_pred[idx].flatten()\n",
    "        \n",
    "        ax.plot(time, actual, 'b-', linewidth=2, label='Actual', alpha=0.8)\n",
    "        ax.plot(time, predicted, 'r--', linewidth=2, label='Predicted', alpha=0.8)\n",
    "        \n",
    "        # Compute metrics\n",
    "        rmse = np.sqrt(np.mean((actual - predicted)**2))\n",
    "        jh_actual = compute_jump_height(actual)\n",
    "        jh_pred = compute_jump_height(predicted)\n",
    "        \n",
    "        ax.set_xlabel('Sample')\n",
    "        ax.set_ylabel('GRF (BW)')\n",
    "        ax.set_title(f'Sample {idx} | RMSE: {rmse:.4f} BW | JH Actual: {jh_actual*100:.1f}cm, Pred: {jh_pred*100:.1f}cm')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.axhline(y=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.axhline(y=0.0, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot random samples\n",
    "plot_grf_comparison(y_val_bw, y_pred_bw, n_samples=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Biomechanical Metrics Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.biomechanics import compute_jump_metrics_batch\n",
    "\n",
    "# Compute metrics for all samples\n",
    "actual_metrics = compute_jump_metrics_batch(y_val_bw)\n",
    "pred_metrics = compute_jump_metrics_batch(y_pred_bw)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Jump Height\n",
    "ax = axes[0]\n",
    "ax.scatter(actual_metrics['jump_height']*100, pred_metrics['jump_height']*100, \n",
    "           alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Identity line\n",
    "jh_min = min(actual_metrics['jump_height'].min(), pred_metrics['jump_height'].min()) * 100\n",
    "jh_max = max(actual_metrics['jump_height'].max(), pred_metrics['jump_height'].max()) * 100\n",
    "ax.plot([jh_min, jh_max], [jh_min, jh_max], 'r--', linewidth=2)\n",
    "\n",
    "# Compute R²\n",
    "ss_res = np.sum((actual_metrics['jump_height'] - pred_metrics['jump_height'])**2)\n",
    "ss_tot = np.sum((actual_metrics['jump_height'] - np.mean(actual_metrics['jump_height']))**2)\n",
    "r2_jh = 1 - ss_res/ss_tot if ss_tot > 0 else 0\n",
    "\n",
    "ax.set_xlabel('Actual Jump Height (cm)')\n",
    "ax.set_ylabel('Predicted Jump Height (cm)')\n",
    "ax.set_title(f'Jump Height (R² = {r2_jh:.3f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Peak Power\n",
    "ax = axes[1]\n",
    "ax.scatter(actual_metrics['peak_power'], pred_metrics['peak_power'],\n",
    "           alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "pp_min = min(actual_metrics['peak_power'].min(), pred_metrics['peak_power'].min())\n",
    "pp_max = max(actual_metrics['peak_power'].max(), pred_metrics['peak_power'].max())\n",
    "ax.plot([pp_min, pp_max], [pp_min, pp_max], 'r--', linewidth=2)\n",
    "\n",
    "ss_res = np.sum((actual_metrics['peak_power'] - pred_metrics['peak_power'])**2)\n",
    "ss_tot = np.sum((actual_metrics['peak_power'] - np.mean(actual_metrics['peak_power']))**2)\n",
    "r2_pp = 1 - ss_res/ss_tot if ss_tot > 0 else 0\n",
    "\n",
    "ax.set_xlabel('Actual Peak Power (W/kg)')\n",
    "ax.set_ylabel('Predicted Peak Power (W/kg)')\n",
    "ax.set_title(f'Peak Power (R² = {r2_pp:.3f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Weight Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, x_sample, layer_idx=-1, head_idx=0):\n",
    "    \"\"\"Visualize attention weights for a single sample.\"\"\"\n",
    "    # Forward pass to get attention weights\n",
    "    _ = model(x_sample[np.newaxis, ...], training=False)\n",
    "    \n",
    "    # Get attention weights from specified layer\n",
    "    attn_weights = model.get_attention_weights(layer_idx)\n",
    "    \n",
    "    # Shape: (1, num_heads, seq_len, seq_len)\n",
    "    attn = attn_weights[0, head_idx].numpy()  # Get first sample, specified head\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(attn, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    ax.set_title(f'Attention Weights (Layer {layer_idx}, Head {head_idx})')\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    return fig, attn\n",
    "\n",
    "# Visualize attention for a random sample\n",
    "sample_idx = np.random.randint(len(X_val))\n",
    "fig, attn = visualize_attention(model, X_val[sample_idx], layer_idx=-1, head_idx=0)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAttention weights shape: {attn.shape}\")\n",
    "print(f\"Attention weights sum (should be ~1 per row): {attn.sum(axis=-1)[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_over_signal(model, x_sample, y_true, y_pred, query_position=250):\n",
    "    \"\"\"Plot attention weights overlaid on signals.\"\"\"\n",
    "    # Get attention weights\n",
    "    _ = model(x_sample[np.newaxis, ...], training=False)\n",
    "    attn_weights = model.get_attention_weights(-1)\n",
    "    \n",
    "    # Average across heads\n",
    "    attn_avg = attn_weights[0].numpy().mean(axis=0)  # (seq_len, seq_len)\n",
    "    \n",
    "    # Get attention for specific query position\n",
    "    attn_query = attn_avg[query_position]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    time = np.arange(500)\n",
    "    \n",
    "    # Input signal\n",
    "    ax = axes[0]\n",
    "    ax.plot(time, x_sample.flatten(), 'b-', linewidth=1.5)\n",
    "    ax.axvline(x=query_position, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylabel('ACC (normalized)')\n",
    "    ax.set_title(f'Input Signal (query position: {query_position})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention weights\n",
    "    ax = axes[1]\n",
    "    ax.fill_between(time, 0, attn_query, alpha=0.7)\n",
    "    ax.axvline(x=query_position, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylabel('Attention Weight')\n",
    "    ax.set_title('Attention Distribution (averaged across heads)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Output comparison\n",
    "    ax = axes[2]\n",
    "    ax.plot(time, y_true, 'b-', linewidth=2, label='Actual', alpha=0.8)\n",
    "    ax.plot(time, y_pred, 'r--', linewidth=2, label='Predicted', alpha=0.8)\n",
    "    ax.axvline(x=query_position, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('GRF (BW)')\n",
    "    ax.set_title('Output Comparison')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Interactive visualization for a sample\n",
    "sample_idx = np.random.randint(len(X_val))\n",
    "plot_attention_over_signal(\n",
    "    model, \n",
    "    X_val[sample_idx], \n",
    "    y_val_bw[sample_idx].flatten(),\n",
    "    y_pred_bw[sample_idx].flatten(),\n",
    "    query_position=300  # Position during propulsion phase\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = evaluate_model(model, X_val, y_val, loader)\n",
    "print_evaluation_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best and Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-sample RMSE\n",
    "sample_rmse = np.array([\n",
    "    np.sqrt(np.mean((y_val_bw[i] - y_pred_bw[i])**2))\n",
    "    for i in range(len(y_val_bw))\n",
    "])\n",
    "\n",
    "# Best predictions (lowest RMSE)\n",
    "best_indices = np.argsort(sample_rmse)[:5]\n",
    "print(\"Best predictions (lowest RMSE):\")\n",
    "for idx in best_indices:\n",
    "    print(f\"  Sample {idx}: RMSE = {sample_rmse[idx]:.4f} BW\")\n",
    "\n",
    "fig = plot_grf_comparison(y_val_bw, y_pred_bw, indices=best_indices)\n",
    "fig.suptitle('Best Predictions', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Worst predictions (highest RMSE)\n",
    "worst_indices = np.argsort(sample_rmse)[-5:]\n",
    "print(\"\\nWorst predictions (highest RMSE):\")\n",
    "for idx in worst_indices:\n",
    "    print(f\"  Sample {idx}: RMSE = {sample_rmse[idx]:.4f} BW\")\n",
    "\n",
    "fig = plot_grf_comparison(y_val_bw, y_pred_bw, indices=worst_indices)\n",
    "fig.suptitle('Worst Predictions', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
